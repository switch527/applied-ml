{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/switch527/applied-ml/blob/main/Hands_on_with_Retrieval_Augmented_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5151afed",
      "metadata": {
        "id": "5151afed"
      },
      "source": [
        "# Retrieval-augmented generation (RAG)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/use_cases/question_answering/qa.ipynb)\n",
        "\n",
        "## Use case\n",
        "Suppose you have some text documents (PDF, blog, Notion pages, etc.) and want to ask questions related to the contents of those documents.\n",
        "\n",
        "LLMs, given their proficiency in understanding text, are a great tool for this.\n",
        "\n",
        "In this walkthrough we'll go over how to build a question-answering over documents application using LLMs.\n",
        "\n",
        "Two very related use cases which we cover elsewhere are:\n",
        "- [QA over structured data](https://python.langchain.com/docs/use_cases/qa_structured/sql) (e.g., SQL)\n",
        "- [QA over code](https://python.langchain.com/docs/use_cases/question_answering/code_understanding) (e.g., Python)\n",
        "\n",
        "![intro.png](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/qa_intro.png?raw=true)\n",
        "\n",
        "## Overview\n",
        "The pipeline for converting raw unstructured data into a QA chain looks like this:\n",
        "1. `Loading`: First we need to load our data. Use the [LangChain integration hub](https://integrations.langchain.com/) to browse the full set of loaders.\n",
        "2. `Splitting`: [Text splitters](/docs/modules/data_connection/document_transformers/) break `Documents` into splits of specified size\n",
        "3. `Storage`: Storage (e.g., often a [vectorstore](/docs/modules/data_connection/vectorstores/)) will house [and often embed](https://www.pinecone.io/learn/vector-embeddings/) the splits\n",
        "\n",
        "![rag_indexing](https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)\n",
        "\n",
        "4. `Retrieval`: The app retrieves splits from storage (e.g., often [with similar embeddings](https://www.pinecone.io/learn/k-nearest-neighbor/) to the input question)\n",
        "5. `Generation`: An [LLM](/docs/modules/model_io/models/llms/) produces an answer using a prompt that includes the question and the retrieved data\n",
        "\n",
        "![reg](https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)\n",
        "\n",
        "\n",
        "\n",
        "## Quickstart\n",
        "\n",
        "Suppose we want a QA app over this [blog post](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives).\n",
        "\n",
        "We can create this in a few lines of code.\n",
        "\n",
        "First set environment variables and install packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e14b744b",
      "metadata": {
        "id": "e14b744b"
      },
      "outputs": [],
      "source": [
        "!pip install langchain chromadb sentence-transformers InstructorEmbedding transformers torch accelerate bitsandbytes ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba5daed6",
      "metadata": {
        "id": "ba5daed6"
      },
      "source": [
        "## Step 1. Load\n",
        "\n",
        "Specify a `DocumentLoader` to load in your unstructured data as `Documents`.\n",
        "\n",
        "A `Document` is a dict with text (`page_content`) and `metadata`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820244ae-74b4-4593-b392-822979dd91b8",
      "metadata": {
        "id": "820244ae-74b4-4593-b392-822979dd91b8"
      },
      "outputs": [],
      "source": [
        "# Load documents\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd2cc9a7",
      "metadata": {
        "id": "fd2cc9a7"
      },
      "source": [
        "### Go deeper\n",
        "- Browse the > 160 data loader integrations [here](https://integrations.langchain.com/).\n",
        "- See further documentation on loaders [here](/docs/modules/data_connection/document_loaders/).\n",
        "\n",
        "## Step 2. Split\n",
        "\n",
        "Split the `Document` into chunks for embedding and vector storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c89a0aa7-1e7e-4557-90e5-a7ea87db00e7",
      "metadata": {
        "id": "c89a0aa7-1e7e-4557-90e5-a7ea87db00e7"
      },
      "outputs": [],
      "source": [
        "# Split documents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a33bd4d",
      "metadata": {
        "id": "0a33bd4d"
      },
      "source": [
        "### Go deeper\n",
        "\n",
        "- `DocumentSplitters` are just one type of the more generic `DocumentTransformers`.\n",
        "- See further documentation on transformers [here](/docs/modules/data_connection/document_transformers/).\n",
        "- `Context-aware splitters` keep the location (\"context\") of each split in the original `Document`:\n",
        "    - [Markdown files](/docs/use_cases/question_answering/document-context-aware-QA)\n",
        "    - [Code (py or js)](docs/integrations/document_loaders/source_code)\n",
        "    - [Documents](/docs/integrations/document_loaders/grobid)\n",
        "\n",
        "## Step 3. Store\n",
        "\n",
        "To be able to look up our document splits, we first need to store them where we can later look them up.\n",
        "\n",
        "The most common way to do this is to embed the contents of each document split.\n",
        "\n",
        "We store the embedding and splits in a vectorstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "000e46f6-dafc-4a43-8417-463d0614fd30",
      "metadata": {
        "id": "000e46f6-dafc-4a43-8417-463d0614fd30"
      },
      "outputs": [],
      "source": [
        "# Embed and store splits\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "# Downloading embedding model\n",
        "embedding_model = HuggingFaceInstructEmbeddings(\n",
        "    model_name = \"hkunlp/instructor-large\",\n",
        "    embed_instruction = \"Represent the document for retrieval: \",\n",
        "    query_instruction = \"Represent the question for retrieving supporting documents: \",\n",
        "    model_kwargs = {'device': 'cuda'}\n",
        ")\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc6f22b0",
      "metadata": {
        "id": "dc6f22b0"
      },
      "source": [
        "### Go deeper\n",
        "- Browse the > 40 vectorstores integrations [here](https://integrations.langchain.com/).\n",
        "- See further documentation on vectorstores [here](/docs/modules/data_connection/vectorstores/).\n",
        "- Browse the > 30 text embedding integrations [here](https://integrations.langchain.com/).\n",
        "- See further documentation on embedding models [here](/docs/modules/data_connection/text_embedding/).\n",
        "\n",
        " Here are Steps 1-3:\n",
        "\n",
        "![lc.png](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/qa_data_load.png?raw=true)\n",
        "\n",
        "## Step 4. Retrieve\n",
        "\n",
        "Retrieve relevant splits for any question using [similarity search](https://www.pinecone.io/learn/what-is-similarity-search/).\n",
        "\n",
        "This is simply \"top K\" retrieval where we select documents based on embedding similarity to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b9fdae-c2bf-4cf6-884f-c19aa07dd975",
      "metadata": {
        "id": "79b9fdae-c2bf-4cf6-884f-c19aa07dd975"
      },
      "outputs": [],
      "source": [
        "# LLM\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "                                             load_in_4bit=True,\n",
        "                                             device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"
      ],
      "metadata": {
        "id": "tbJ0yHZJl4FY"
      },
      "id": "tbJ0yHZJl4FY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "test_prompt = \"\"\"### Instruction: Tell me what you know about the SR-71 Blackbird.\n",
        "\n",
        " ### Answer:\n",
        " \"\"\"\n",
        "\n",
        "encoded_instruction = tokenizer(test_prompt,\n",
        "                                return_tensors=\"pt\",\n",
        "                                add_special_tokens=True)\n",
        "\n",
        "model_inputs = encoded_instruction.to(device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs,\n",
        "                               max_new_tokens=1000,\n",
        "                               do_sample=True,\n",
        "                               pad_token_id=tokenizer.eos_token_id)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "id": "4ABiw54ymB0O"
      },
      "id": "4ABiw54ymB0O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "text_generation_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=1000,\n",
        ")\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "4B5b3KJ6mZH6"
      },
      "id": "4B5b3KJ6mZH6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92c0f3ae-6ab2-4d04-9b22-1963b96b9db5",
      "metadata": {
        "id": "92c0f3ae-6ab2-4d04-9b22-1963b96b9db5"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Prompt template\n",
        "qa_template = \"\"\"<s>[INST] You are a helpful assistant.\n",
        "Use the following context to answer the question below.\n",
        "If you cannot answer a questions based on the provided context, respond with \"Unable to answer the question based on the provided context\".\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question} [/INST] </s>\n",
        "\"\"\"\n",
        "\n",
        "# Create a prompt instance\n",
        "QA_PROMPT = PromptTemplate.from_template(qa_template)\n",
        "\n",
        "# Custom QA Chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3b0f36-7b56-49c0-8e40-a1aa9ebcbf24",
      "metadata": {
        "id": "0d3b0f36-7b56-49c0-8e40-a1aa9ebcbf24"
      },
      "outputs": [],
      "source": [
        "# Enter a Question\n",
        "\n",
        "\n",
        "# Query Mistral 7B Instruct model\n",
        "\n",
        "\n",
        "# Print your result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. Evaluate\n",
        "\n",
        "We'll start by generating some sample question and answer pairs:"
      ],
      "metadata": {
        "id": "lb6HPaBudU__"
      },
      "id": "lb6HPaBudU__"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_questions = [\n",
        "    \"What are the three main steps in the canonical training pipeline for modern transformer-based LLMs?\",\n",
        "    \"What is the purpose of the pretraining phase in the LLM training pipeline?\",\n",
        "    \"What does RLHF stand for, and how does it contribute to the LLM training pipeline?\",\n",
        "    \"How does RLHF Step 2 create a reward model in the RLHF pipeline?\",\n",
        "    \"What are some alternatives to RLHF discussed in the article, and how do they differ?\",\n",
        "]\n",
        "\n",
        "eval_answers = [\n",
        "    \"The three main steps are Pretraining, Supervised finetuning, and Alignment.\",\n",
        "    \"The Eagles are going to win the Super Bowl this season.\",\n",
        "    \"RLHF stands for Reinforcement Learning with Human Feedback. It contributes by aligning the LLM with human preferences, improving its helpfulness and safety.\",\n",
        "    \"In RLHF Step 2, for each prompt, multiple responses are generated from the finetuned LLM, and individuals rank these responses based on their preference, forming a dataset for creating a reward model.\",\n",
        "    \"Some alternatives include Constitutional AI, Hindsight Instruction Labeling, Direct Preference Optimization, Contrastive Preference Learning, Reinforced Self-Training, and Reinforcement Learning with AI Feedback (RLAIF). Each alternative has distinct approaches, such as self-training based on rules, supervised finetuning with relabeling, direct use of cross entropy loss, contrastive loss learning, self-training with offline dataset generation, and reinforcement learning with AI-generated feedback.\",\n",
        "]\n",
        "\n",
        "examples = [\n",
        "    {\"query\": q, \"ground_truths\": [eval_answers[i]]}\n",
        "    for i, q in enumerate(eval_questions)\n",
        "]"
      ],
      "metadata": {
        "id": "FilT7RzPhkYV"
      },
      "id": "FilT7RzPhkYV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure you have you OpenAI API key ready\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = # Enter API Key here"
      ],
      "metadata": {
        "id": "OpO7hQqcvrUS"
      },
      "id": "OpO7hQqcvrUS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.langchain.evalchain import RagasEvaluatorChain\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_relevancy,\n",
        "    context_recall,\n",
        ")\n",
        "\n",
        "# create evaluation chains\n",
        "faithfulness_chain = RagasEvaluatorChain(metric=faithfulness)\n",
        "answer_rel_chain = RagasEvaluatorChain(metric=answer_relevancy)\n",
        "context_rel_chain = RagasEvaluatorChain(metric=context_relevancy)\n",
        "context_recall_chain = RagasEvaluatorChain(metric=context_recall)"
      ],
      "metadata": {
        "id": "fPJwldEVkzP3"
      },
      "id": "fPJwldEVkzP3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain({\"query\": eval_questions[0]})\n",
        "print(result[\"result\"])"
      ],
      "metadata": {
        "id": "ZPlfCzG4isPO"
      },
      "id": "ZPlfCzG4isPO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = faithfulness_chain(result)\n",
        "eval_result[\"faithfulness_score\"]"
      ],
      "metadata": {
        "id": "_sE_iOlasKGg"
      },
      "id": "_sE_iOlasKGg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the queries as a batch for efficiency\n",
        "predictions = qa_chain.batch(eval_questions)"
      ],
      "metadata": {
        "id": "4b0HRSa7wkyM"
      },
      "id": "4b0HRSa7wkyM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "Dtvau1STyVRr"
      },
      "id": "Dtvau1STyVRr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "snHKb0EGyoVs"
      },
      "id": "snHKb0EGyoVs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}